<img src="https://github.com/ChaoningZhang/Awesome-segment-anything-model/blob/main/fig/masks2.jpg" width="900" height="500" alt="Segment Anything" />

# Awesome-segment-anything-model :octocat:

**Original paper link:https://ai.facebook.com/research/publications/segment-anything/**

**Original repo link:https://github.com/facebookresearch/segment-anything**

# Related Papers

### [Track Anything: Segment Anything Meets Videos](https://arxiv.org/pdf/2304.11968)

### [Segment Anything in Medical Images](https://arxiv.org/pdf/2304.12306)

### [Segment Anything in Non-Euclidean Domains: Challenges and Opportunities](https://arxiv.org/pdf/2304.11595)

### [Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model](https://arxiv.org/pdf/2304.11332)

### [Downstream Task Self-Supervised Learning for Object Recognition and Tracking](https://search.proquest.com/openview/dd0fd0e68635132bbebbd95786f4f9e8/1?pq-origsite=gscholar&cbl=18750&diss=y)

### [Advances in Deep Concealed Scene Understanding](https://arxiv.org/pdf/2304.11234)

### [Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models](https://arxiv.org/pdf/2304.10597)

### [Any-to-Any Style Transfer](https://arxiv.org/pdf/2304.09728)

### [Anything-3D: Towards Single-view Anything Reconstruction in the Wild](https://arxiv.org/pdf/2304.10261)

### [SAM Fails to Segment Anything?--SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, and More](https://arxiv.org/pdf/2304.09148)

### [Accuracy of Segment-Anything Model (SAM) in medical image segmentation tasks](https://arxiv.org/abs/2304.09324)

### [Learning to "Segment Anything" in Thermal Infrared Images through Knowledge Distillation with a Large Scale Dataset SATIR](https://arxiv.org/abs/2304.07969)

This paper presents a framework for utilizing the Segment Anything Model (SAM) to generate pseudo labels for pretraining thermal infrared image segmentation tasks, as well as a large scale thermal infrared segmentation dataset. This approach is an effective solution to work with large models in special fields where label annotation is challenging, and has been demonstrated to improve the accuracy of segmentation results beyond the SOTA ImageNet pretrained model.

### [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)(2023/04/13)

This paper presents the first attempt of using GPT-4 to generate multimodal language-image instruction-following data. This data is used to create the Large Language and Vision Assistant (LLaVA), an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding. Early experiments show that LLaVA exhibits impressive chat abilities and yields a 85.1% relative score compared to GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.

### [When SAM Meets Medical Images: An Investigation of Segment Anything Model (SAM) on Multi-phase Liver Tumor Segmentation](https://arxiv.org/pdf/2304.08506.pdf)(2023/04/21)

This paper investigates the capability of the Segment Anything Model (SAM) for medical image analysis, specifically for multi-phase liver tumor segmentation (MPLiTS). Experiments demonstrate that there is a gap between SAM and expected performance, however, the qualitative results show that SAM is a powerful annotation tool for interactive medical image segmentation.

### [Can SAM Segment Polyps?](https://arxiv.org/pdf/2304.07583.pdf)(2023/04/15)

This report evaluates the performance of the recently released Meta AI Research Segment Anything Model (SAM) in polyp segmentation under unprompted settings. Results of the evaluation are available publicly at https://github.com/taozh2017/SAMPolyp and could provide insights to advance the polyp segmentation field and inspire further research.

### [SAM Fails to Segment Anything? -SAM-Adaptor: Adapting SAM in Underperformed Scenes](https://www.researchgate.net/publication/370025539_SAM_Fails_to_Segment_Anything_-SAM-Adaptor_Adapting_SAM_in_Underperformed_Scenes)(2023/04/15)

This study proposes SAM-Adaptor, an image segmentation network incorporating domain-specific information or visual prompts into the large pre-trained model Segment Anything (SAM). Experimental findings show that SAM-Adaptor can significantly elevate SAM's performance in challenging tasks, such as shadow detection and camouflaged object detection, and even achieve state-of-the-art performance. It has potential applications in various fields, including medical image processing, agriculture, and remote sensing.

### [Inpaint Anything: Segment Anything Meets Image Inpainting](https://arxiv.org/pdf/2304.06790.pdf)(2023/04/13)

This paper presents Inpaint Anything (IA), a mask-free image inpainting system based on Segment-Anything Model (SAM). IA has three features: (I) Remove Anything; (ii) Fill Anything with text-based prompts; and (iii) Replace Anything. 

### [Brain Extraction comparing Segment Anything Model (SAM) and FSL Brain Extraction Tool](https://arxiv.org/pdf/2304.04738.pdf)(2023/04/15)

This paper compares the Segment Anything Model (SAM) with FSL's Brain Extraction Tool (BET) for brain extraction on different brain scans. Results show that SAM outperforms BET in various evaluation parameters, especially with signal inhomogeneities, non-isotropic voxel resolutions, or lesions near the brain's outer regions and meninges. SAM's superior performance suggests its potential as a more accurate, robust, and versatile tool for brain extraction and segmentation applications.

<img src="https://github.com/ChaoningZhang/Awesome-segment-anything-model/blob/main/fig/IIXSUPR%24(V4%60EXXTKZ3EO%7B3.png" width="550" height="400" alt="Segment Anything" />

### [SAM Struggles in Concealed Scenes--Empirical Study on" Segment Anything"](https://arxiv.org/pdf/2304.06022.pdf)(2023/04/15)

In this report, three concealed scenes (camouflaged animals, industrial defects, and medical lesions) have been tested to evaluate SAM's performance under unprompted settings. The main observation is that SAM struggles to accurately identify objects in these scenes.

<img src="https://github.com/ChaoningZhang/Awesome-segment-anything-model/blob/main/fig/HAAC32DMHSH(M_NQ87EDK%7B8.png" width="780" height="400" alt="Segment Anything" />

### [Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications](https://arxiv.org/pdf/2304.05750.pdf)(2023/04/13)

This work investigates the performance of the Segment Anything Model (SAM) pre-trained on SA-1B across various applications, such as natural images, agriculture, manufacturing, remote sensing, and healthcare. The benefits and limitations of SAM are analyzed and discussed, with an outlook on future development of segmentation tasks. This provides a comprehensive view of SAM in practice, which will facilitate future research activities.

<img src="https://github.com/ChaoningZhang/Awesome-segment-anything-model/blob/main/fig/J%40%5B_6Z6DP7GI8QYI%7D%60PHMKD.png" width="780" height="400" alt="Segment Anything" />

### [SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM](https://arxiv.org/pdf/2304.05622.pdf)(2023/04/12)

This paper introduces the Segment Any Medical Model (SAMM), a 3D Slicer extension of the Segment Anything Model (SAM) for medical image segmentation. SAMM has demonstrated good promptability and generalizability and can infer masks in nearly real-time with 0.6-second latency. The open-source SAMM and its demonstrations are available on GitHub.

<img src="https://github.com/ChaoningZhang/Awesome-segment-anything-model/blob/main/fig/504(G3LC1C)%24%602ASDTIKONC.png" width="780" height="400" alt="Segment Anything" />

### [Can SAM Segment Anything? When SAM Meets Camouflaged Object Detection](https://arxiv.org/pdf/2304.04709)(2023/04/11)

This paper investigates how well SAM performs in the task of Camouflaged Object Detection (COD) and compares SAM's performance to 22 state-of-the-art COD methods. 

<img src="https://github.com/ChaoningZhang/Awesome-segment-anything-model/blob/main/fig/9GCIT)PHA%5BU%25HEEE%60Y0B%7D_8.png" width="780" height="400" alt="Segment Anything" />

### [Segmenting Anything Also Detect Anything](https://wwwww.easychair.org/publications/preprint_download/HVhP)(2023/04/10)

This paper proposes the use of computer vision macromodels (SAMs) to guide semi-automated annotation of data in the domain of specific object detection, and the High Fine Grain Fill-in Augmentation (HFGFA) method for visual image data augmentation. These approaches have been shown to improve model generalisation and open world object detection capabilities.

<img src="https://github.com/ChaoningZhang/Awesome-segment-anything-model/blob/main/fig/~IA%5DE%7D08%5D(%24WKHYF535IRSX.png" width="780" height="400" alt="Segment Anything" />

### [Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging](https://arxiv.org/pdf/2304.04155.pdf)(2023/04/09)

This study evaluates the SAM model's performance on whole slide imaging (WSI) tasks such as tumor segmentation, non-tumor tissue segmentation, and cell nuclei segmentation. The results suggest that the model performs well for large objects, but does not consistently perform well for dense instance object segmentation. Identified limitations for digital pathology include image resolution, multiple scales, prompt selection, and model fine-tuning. Future work should explore few-shot fine-tuning with images from downstream pathological segmentation tasks to improve performance.

<img src="https://github.com/ChaoningZhang/Awesome-segment-anything-model/blob/main/fig/E54BTV%40K3Q3O%40%25K8%7DW1%5DQ%7BP.png" width="780" height="400" alt="Segment Anything" />

# Related Projects

* [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything#grounded-segment-anything)
* [GroundedSAM-zero-shot-anomaly-detection](https://github.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection)
* [Segment and Track Anything (SAM-Track)](https://github.com/z-x-yang/Segment-and-Track-Anything)
* [SEEM: Segment Everything Everywhere All at Once](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)
* [IEA: Image Editing Anything](https://github.com/feizc/IEA)
* [Semantic Segment Anything (SSA)](https://github.com/fudan-zvg/Semantic-Segment-Anything)
* [Segment Anything with Clip](https://github.com/Curt-Park/segment-anything-with-clip)
* [Magic Copy](https://github.com/kevmo314/magic-copy)
* [Edit Anything by Segment-Anything](https://github.com/sail-sg/EditAnything)
* [Prompt-Segment-Anything](https://github.com/RockeyCoss/Prompt-Segment-Anything)
* [SAM-RBox](https://github.com/Li-Qingyun/sam-mmrotate)
* [Segment-anything-with-image-captioning](https://github.com/bnabis93/segment-anything-image-search)
* [Open-vocabulary-Segment-Anything](https://github.com/ngthanhtin/owlvit_segment_anything)
* [SegDrawer](https://github.com/lujiazho/SegDrawer)
* [Label-Anything-Pipeline](https://github.com/Yuqifan1117/Labal-Anything-Pipeline)
* [MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors](https://github.com/BingfengYan/VISAM)
* [Segment Anything Model (SAM) in Napari](https://github.com/MIC-DKFZ/napari-sam)
* [Inpaint Anything: Segment Anything Meets Image Inpainting](https://github.com/geekyutao/Inpaint-Anything)
* [Segment Anything EO tools](https://github.com/aliaksandr960/segment-anything-eo)
* [napari-segment-anything](https://github.com/JoOkuma/napari-segment-anything)
* [Grounded Segment Anything: From Objects to Parts](https://github.com/Cheems-Seminar/grounded-segment-any-parts)
* [AnyLabeling](https://github.com/vietanhdev/anylabeling)
* [Caption-Anything](https://github.com/ttengwang/Caption-Anything)
* [Segment-Anything-U-Specify](https://github.com/MaybeShewill-CV/segment-anything-u-specify)
* [Optical Character Recognition with Segment Anything (OCR-SAM)](https://github.com/yeungchenwa/OCR-SAM)
* [grounded-segment-anything-colab](https://github.com/camenduru/grounded-segment-anything-colab)
* [SAM Medical Imaging](https://github.com/amine0110/SAM-Medical-Imaging)
* [Image.txt: Transform Image Into Unique Paragraph](https://github.com/showlab/Image2Paragraph)
* [LIME-SAM](https://github.com/jaydeep-work/LIME-SAM)
* [A simple demo for SAM+MMDetection](https://github.com/liuyanyi/sam-with-mmdet)
* [3D-Box via Segment Anything](https://github.com/dvlab-research/3D-Box-Segment-Anything)
* [Anything-3D](https://github.com/Anything-of-anything/Anything-3D)
* [Transfer-Any-Style](https://github.com/Anything-of-anything/Transfer-Any-Style)
* [Paint-Anything](https://github.com/Huage001/Paint-Anything)
* [Track-Anything](https://github.com/gaomingqi/Track-Anything)
